\documentclass{article}

% ready for submission
\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}}

\title{Probability in high dimensions}

\date{April 21, 2019}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Aidan Rocke\\
  \texttt{aidanrocke@gmail.com} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

\begin{document}

\maketitle

\begin{abstract}
   While running a computer simulation that involved the ratio of two random variables whose denominator was a symmetric random variable centred at zero, I observed that this fact never caused the simulation to crash. This made me curious and I discovered that this interesting numerical observation turned out to be a theorem. While this is not a fundamental discovery, it is one of many experiences that convinced me of the increasingly important role computers will play in the discovery of mathematical theorems in the future. 
   \end{abstract}

\section{Analysis of a special case}

In order to make progress, I decided to start by analysing the special case of $a_i \sim \mathcal{U}(\{-1,1\})$ where:

\begin{equation}
\forall n \in \mathbb{N}, P(a_n=1)=P(a_n=-1)=\frac{1}{2}
\end{equation}

\begin{equation}
S_0  = \{ (a_n)_{n=1}^N \in \\{-1,1\\} : \sum_n a_n = 0\}
\end{equation}

Knowing that $S_0$ is non-empty only if we have parity of positive and negative terms, we may deduce that:

\begin{equation}
S_0 \neq \emptyset \iff N \in 2\mathbb{N}
\end{equation}

For the above reason, I focused my analysis on the following sequence:

\begin{equation}
u_N = P(\sum_{n=1}^{2N} a_n = 0)= \frac{{2N \choose N}}{2^{2N}} = \frac{(2N)!}{2^{2N}(N!)^2}
\end{equation}

\subsection{Proof that $u_N$ is decreasing}

We can demonstrate that $u_N$ is strictly decreasing by considering the ratio:

\begin{equation}
\frac{u_{N+1}}{u_N}=\frac{\frac{(2N+2)!}{2^{2N+2}((N+1)!)^2}}{\frac{(2N)!}{2^{2N}(N!)^2}}=\frac{(2N+2)(2N+1)}{4(N+1)^2}=\frac{2N+1}{2N+2} < 1
\end{equation}

Now, with (5) we have what is necessary to show that:

\begin{equation}
\lim_{n \to \infty} u_N = 0
\end{equation}

\newpage

\subsection{Analysis of the limit $\lim\limits_{N \to \infty} u_N$}

Using (5) we may also derive a recursive definition of $u_N$:

\begin{equation}
u_{N+1}=\frac{2N+1}{2N+2} \cdot u_N
\end{equation}

and given that $u_0=1$ we have:

\begin{equation}
u_{N}=\prod_{n=0}^{N-1} \frac{2n+1}{2n+2}= \frac{1}{3} \cdot \frac{3}{4} \cdot \frac{5}{6} \cdot ...
\end{equation}

At this point we can make the useful observation:

\begin{equation}
\lim_{N \to \infty} u_N = 0 \implies \lim_{N \to \infty} - \ln u_N = \infty
\end{equation}

\subsection{Proof that $\lim\limits_{N \to \infty} u_N=0$}

By combining (7) and (9) we find that:

\begin{equation}
-\ln u_N = -\ln \prod_{n=0}^{N-1} \frac{2n+1}{2n+2}= \sum_{n=0}^{N-1} \ln \frac{2n+2}{2n+1}= \sum_{n=0}^{N-1} \ln \big(1+\frac{1}{2n+1}\big)
\end{equation}

and we note that when $n\in \mathbb{N}$ is large:

\begin{equation}
\ln \big(1+\frac{1}{n}\big) \approx \frac{1}{n}
\end{equation}

Now, from (11) it follows that:

\begin{equation}
\sum_{n=1}^\infty \frac{1}{2n+1} = \infty \implies \sum_{n=0}^{\infty} \ln \big(1+\frac{1}{2n+1}\big) = \infty
\end{equation}

So we may conclude that (6) is indeed true. In some sense, when $n$ is large we may expect to observe the expected value
with vanishing probability. This does not contradict the weak law of large numbers however since the weak law only states that the sample average should converge to the expected value whereas in our analysis we are comparing the sample sum to the expected value. 

\subsection{Discussion}

A natural question that follows is whether the above method may be used to handle other cases. Let's consider $a_i \sim \mathcal{U}(\{-1,0,1\})$ where:

\begin{equation}
\forall n \in \mathbb{N}, P(a_n=1)=P(a_n=0)=P(a_n=-1)=\frac{1}{3}
\end{equation}

so we may define:

\begin{equation}
u_N = P(\sum_{n=1}^{4N} a_n = 0)= \frac{(4N)!}{3^{4N}} \sum_{k=1}^N \frac{1}{(2k)!^2 (4N-4k)!}
\end{equation}

I actually tried to analyse the combinatorics of this sequence but realised that even if I managed to show that this sequence converged to zero,
it wasn't clear how this method would manage to handle the most general setting, the case of all integer dimensions $N \in \mathbb{N}$, and it didn't
appear to be very effective in terms of the number of calculations per case. For the more general case, a different approach would be necessary. 

\section{A more general case}

\subsection{A random walk on $\mathbb{Z}$}

Let's suppose $a_i \sim \mathcal{U}([-N,N])$ where $[-N,N] \subset \mathbb{Z}$. We may then define:

\begin{equation}
S_n = \sum_{i=1}^n a_i
\end{equation}

Due to the i.i.d. assumption we have:

\begin{equation}
\mathbb{E}\big[S_n\big]= n \cdot \mathbb{E}\big[a_i\big]=0
\end{equation}

We may now define:

\begin{equation}
u_n = P(S_n=0)
\end{equation}

and ask whether $u_n$ is decreasing. In other words, what is the probability that we observe the expected value as $n$ becomes large?

\subsection{Small and Large deviations}

It's useful to observe the following nested structure:

\begin{equation}
\forall k \in [0,N], \{\lvert S_n \rvert \leq k\} \subset \{\lvert S_n \rvert \leq k+1 \}
\end{equation}

From (18), we may deduce that:

\begin{equation}
P(\lvert S_n \rvert \leq N) + P(\lvert S_n \rvert > N) = 1
\end{equation}

So we are now ready to define the probability of a 'small' deviation:

\begin{equation}
	\alpha_n = P(\lvert S_n \rvert \leq N)
\end{equation}

as well as the probability of 'large' deviations:

\begin{equation}
	\beta_n = P(\lvert S_n \rvert > N)
\end{equation}

Additional motivation for analysing $\alpha_n$ and $\beta_n$ arises from:

\begin{equation}
P(S_{n+1} = 0 | \lvert S_n \rvert > N) = 0
\end{equation}

\begin{equation}
P(S_{n+1} = 0 | \lvert S_n \rvert \leq N) = \frac{1}{2N+1}
\end{equation}

Furthermore, by the law of total probability we have:

\begin{equation}
\begin{split}
P(S_{n+1}=0) & = P(S_{n+1}=0|\lvert S_n \rvert \leq N) \cdot P(\lvert S_n \rvert \leq N) + P(S_{n+1}=0|\lvert S_n \rvert > N) \cdot P(\lvert S_n \rvert > N) \\\
& = P(S_{n+1}=0|\lvert S_n \rvert \leq N) \cdot P(\lvert S_n \rvert \leq N) \\\
& = \frac{P(\lvert S_n \rvert \leq N)}{2N+1}
\end{split}
\end{equation}

\newpage 

\subsection{A remark on symmetry}

It's useful to note the following alternative definitions of $\alpha_n$ and $\beta_n$ that emerge
due to symmetries intrinsic to the problem:

\begin{equation}
	\beta_n = P(\lvert S_n \rvert > N) = 2 \cdot P(S_n > N) = 2 \cdot P(S_n < -N)
\end{equation}

\begin{equation}
	\alpha_n = P(\lvert S_n \rvert \leq N) = 1-2 \cdot P(S_n > N)=1-2 \cdot P(S_n < -N)
\end{equation}

\subsection{The case of $n=1$ and $n=2$}

Given that $S_0=0$:

\begin{equation}
	P(S_1=0)=\frac{P(\lvert S_0 \rvert \leq N)}{2N+1}= \frac{1}{2N+1}
\end{equation}

As for the case of $n=2$:

\begin{equation}
	P(\lvert S_2 \rvert \leq N) =1 \implies P(S_2=0) = \frac{1}{2N+1}
\end{equation}

\subsection{The case of $n=3$}

The case of $n=3$ requires that we calculate:

\begin{equation}
\begin{split}
P(S_{2} > N) & = \sum_{i=1}^N P(S_{2} > N| S_1 = i) \cdot P( S_1 = i) \\\
& = \frac{1}{2N+1} \sum_{i=1}^N (\frac{1}{2N+1} + ... + \frac{N}{2N+1})  \\\
& = \frac{N \cdot (N-1)}{2 \cdot (2N+1)^2}
\end{split}
\end{equation}

and using (29) we may derive $P(S_{2} \leq N)$:

\begin{equation}
\begin{split}
P(S_{2} \leq N) & = 1 - 2 \cdot P(S_{2} > N) \\\
& = 1- \frac{N \cdot (N-1)}{(2N+1)^2}   \\\
& = \frac{3N^2+5N+1}{(2N+1)^2} \sim \frac{3}{4}
\end{split}
\end{equation}

and so for $n=3$ we have:

\begin{equation}
\begin{split}
P(S_{3} = 0) & = P(S_{3} = 0 | \lvert S_2 \rvert \leq N) \cdot P(\lvert S_2 \rvert \leq N)  \\\
& =  \frac{3N^2+5N+1}{(2N+1)^3} \sim \frac{3}{8N}
\end{split}
\end{equation}

\newpage

\subsection{Average drift or why $P(S_n=k) > P(S_n = k+1)$}

It's useful to note that we may decompose $n$ into:

\begin{equation}
n = \hat{n} + n_z
\end{equation}

where $\hat{n}$ represents the total number of positive and negative terms, allowing us to ignore the trivial
contribution of zero terms $n_z$.

For the above reason, it's convenient to decompose $S_n$ into:

\begin{equation}
S_n = S_n^+ + S_n^{-}
\end{equation}

where $S_n^+$ defines the sum of the positive terms and $S_n^{-}$ defines the sum
of the negative terms.

By grouping the terms in the manner of (33) we may observe that when
$\hat{n}$ is large the average positive/negative step length is given by:

\begin{equation}
\Delta = \frac{N}{2}
\end{equation}

so that if $\tau$ positive steps and $\hat{n}-\tau$ negative steps are taken:

\begin{equation}
\mathbb{E}[S_n^+] = \tau \cdot \Delta
\end{equation}

\begin{equation}
\mathbb{E}[S_n^-] = (\hat{n}-\tau) \cdot (-\Delta)
\end{equation}

\begin{equation}
\mathbb{E}[S_n] =  \mathbb{E}[S_n^+] + \mathbb{E}[S_n^-] = \Delta \cdot (2\tau-\hat{n})
\end{equation}

and we note that:

\begin{equation}
\mathbb{E}[S_n] \geq 0 \implies \tau \geq \lfloor \frac{\hat{n}}{2} \rfloor
\end{equation}

Furthermore, due to symmetry:

\begin{equation}
P(\lvert S_n \rvert =k) > P(\lvert S_n \rvert =k+1) \iff P(S_n =k) > P(S_n =k+1)
\end{equation}

so it suffices to demonstrate $P(S_n =k) > P(S_n =k+1)$.

In order to proceed with our demonstration we choose $\tau \in [\lfloor \frac{\hat{n}}{2} \rfloor + 1,\hat{n}N-1]$ and find
that $P$ has a monotone relationship with the binomial distribution:

\begin{equation}
P(S_{n}  = \lfloor \Delta \cdot (2\tau-\hat{n}) \rfloor)  \propto {\hat{n} \choose \tau} \frac{1}{2^{\hat{n}}}
\end{equation}

where $\tau \geq \lfloor \frac{\hat{n}}{2} \rfloor$ implies that:

\begin{equation}
\forall k \geq 0, \frac{P(S_n=k)}{P(S_n=k+1)} \sim \frac{(\tau+1)!(\hat{n}-\tau-1)!}{\tau!(\hat{n}-\tau)!} = \frac{\tau+1}{\hat{n}-\tau} > 1
\end{equation}

which holds for all $n_z \leq n$.

\newpage

\subsection{Proof that $u_n$ is decreasing}

Given (24) we may derive the following ratio:

\begin{equation}
	\frac{u_{n+1}}{u_n} = \frac{P(\lvert S_n \rvert \leq N)}{(2N+1) \cdot P(S_n = 0)}
\end{equation}

So in order to prove that $u_n$ is decreasing we must show that:

\begin{equation}
	P(\lvert S_n \rvert \leq N) < (2N+1) \cdot P(S_n=0)
\end{equation}

and we note that this follows immediately from (42) since:

\begin{equation}
	P(\lvert S_n \rvert \leq N) = 2 \sum_{k=1}^N P(S_n=k) + P(S_n=0) < (2N+1) \cdot P(S_n=0)
\end{equation}

\subsection{Proof that $\lim\limits_{n \to \infty} u_n = \lim\limits_{n \to \infty} \alpha_n = 0$}

Now, given (44) we may define:

\begin{equation}
\forall N \in \mathbb{N}, q_n = \frac{P(\lvert S_n \rvert \leq N)}{(2N+1)P(S_n=0)} < 1
\end{equation}

Furthermore, using the concentration phenomenon (40) we may show that when $n$ is large:

\begin{equation}
P(S_{2n}=0) \sim \frac{1}{2^{2n}} {2n \choose n} \sim \frac{1}{2^{2n}} \frac{\sqrt{4 \pi n} (\frac{2n}{e})^{2n}}{2 \pi n (\frac{n}{e})^{2n}} \sim \frac{1}{\sqrt{\pi n}}
\end{equation}

From this we may deduce that $q_n$ is decreasing:


\begin{equation}
\prod_{n=1}^\infty q_n = \prod_{n=1}^\infty \frac{P(S_{n+1}=0)}{P(S_n=0)} = \lim_{n \to \infty} \frac{P(S_{n+1}=0)}{P(S_1=0)} = 0
\end{equation}

Likewise, given that:

\begin{equation}
\alpha_n = P(\lvert S_n \rvert \leq N) = (2N+1) \cdot P(S_{n+1}=0)
\end{equation}

we may conclude that large deviations are exponentially more likely as $n$ becomes large:

\begin{equation}
\lim_{n \to \infty} \alpha_n = \lim_{n \to \infty} (2N+1) \cdot P(S_{n+1}=0) = 0
\end{equation}

\begin{equation}
\lim_{n \to \infty} \beta_n = \lim_{n \to \infty} P(\lvert S_n \rvert > N) = \lim_{n \to \infty} 1 - \alpha_n = 1
\end{equation}

A geometric interpretation of the last two limits is that the ratio of the largest hyperplane intersection of the discrete hypercube relative to the total volume of the discrete hypercube shrinks as the dimension of the hypercube tends to infinity.

\section{Discussion}

One interesting lesson from this experience besides the growing importance of 
experimental mathematics is that random structures, in this case a random walk, 
are useful for analysing high-dimensional objects. To what degree this is because
our intuition for high dimensional structures isn't much better than random, I don't yet have the answer. 

\end{document}